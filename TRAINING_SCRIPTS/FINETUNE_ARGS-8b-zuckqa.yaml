# Model
model_name: "amuvarma/8b-2m-pretrain"  # Replace with your base model must be compatible with the tokenizer and transformers library
tokenizer_name: "meta-llama/Llama-3.1-8B"

# Training Args
epochs: 1
batch_size: 1
number_processes: 8
pad_token: 128263
save_steps: 2000
learning_rate: 5.0e-6

# Datasets
text_QA_dataset: "amuvarma/5k-qa-pairs-tttts"
TTS_dataset: "amuvarma/va-320k-330k-snac-no-identity-QA_TTTTS"

# Naming and paths
save_folder: "checkpoints"
project_name: "zuck-qa-tune-8b"
run_name: "20-1-r1"

resize_dataset: false