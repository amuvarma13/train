model_name: "amuvarma/3b-10m-pretrain-full"
tokenizer_name: "meta-llama/Llama-3.1-8B"

# Training Args
epochs: 1
batch_size: 1
number_processes: 1
pad_token: 128263
save_steps: 10000
learning_rate: 9.0e-6

# Datasets
# text_QA_dataset: "amuvarma/all-tagged-qa-6k-proc-2g"
# TTS_dataset: "amuvarma/luna-all-raw-snacced-TTS-2g"

text_QA_dataset: "amuvarma/hybrid-text-convo-labels"
TTS_dataset: "amuvarma/luna-tags_tts_convos-coll-ttsed-proc"
# Naming and paths
save_folder: "checkpoints_hlr"
project_name: "luna-tts-text"
run_name: "r0-9e6"

resize_dataset: false