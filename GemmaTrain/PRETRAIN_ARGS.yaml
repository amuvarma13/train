# Model
t  # Replace with your base model must be compatible with the tokenizer and transformers library
tokenizer_name: "google/gemma-2-9b-it"

# Training Args
epochs: 1
batch_size: 1
number_processes: 24
pad_token: 128263
save_steps: 12000
learning_rate: 5.0e-5

# Datasets
text_QA_dataset: "amuvarma/all-texts-2048-iids"
TTS_dataset: "amuvarma/emilia-snac-merged-all-gemma-TTS-grouped-2656"

# Naming and paths
save_folder: "checkpoints"
project_name: "gemma-batched-pretrain"
run_name: "r1"
