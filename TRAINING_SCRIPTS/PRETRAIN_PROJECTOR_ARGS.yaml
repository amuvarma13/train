# Model
model_name: "amuvarma/zuckconvotune-3b-2m-checkpoint-124"  # Replace with your base model must be compatible with the tokenizer and transformers library
tokenizer_name: "meta-llama/Llama-3.2-3B"

# Training Args
epochs: 1
batch_size: 1
pad_token: 128263
save_steps: 10000
learning_rate: 2.0e-3
gradient_accumulation_steps: 1

# Datasets
text_QA_dataset: "amuvarma/qa_pairs_regular-QA_TTTTS"
TTS_dataset: "amuvarma/snacced-flat-zuck-convo-StTtS"

# Naming and paths
save_folder: "checkpoints"
project_name: "zuck-convo-tune-3b"
run_name: "15-1-3b-2m-r0"

resize_dataset: false