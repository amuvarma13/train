# Model
model_name: "amuvarma/3b-10m-pretrain-full"  # Replace with your base model must be compatible with the tokenizer and transformers library
tokenizer_name: "meta-llama/Llama-3.2-3B"

# Training Args
epochs: 1
batch_size: 1
number_processes: 2
pad_token: 128263
save_steps: 7000
learning_rate: 5.0e-5

# Datasets
text_QA_dataset: "amuvarma/10k-qa-proc"
TTS_dataset: "amuvarma/luna-6k-snacced-TTTTS"
motion_dataset: "amuvarma/humanml3d-flat-train-t5-toks-grouped"

# Naming and paths
save_folder: "checkpoints"
project_name: "luna-speech-motion-3b"
run_name: "9-2-r1"

resize_dataset: false